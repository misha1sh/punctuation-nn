{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Either FairScale or torch distributed is not available, MixtureOfExperts will not be exposed. Please install them if you would like to use MoE\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import dill\n",
    "import torch\n",
    "from inference import torch_model_runner\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(INTERNAL_EMBEDDING_SIZE2, INTERNAL_EMBEDDING_SIZE2 // 2, \n",
    "                            num_layers=1, batch_first=True, bidirectional=True)\n",
    "    def forward(self, x):\n",
    "        return self.lstm(x)[0]\n",
    "    \n",
    "prefix = \"results writers big\"\n",
    "with open(f\"{prefix}/some_model_CLASS.dill\", \"rb\") as file:\n",
    "  Model = dill.load(file)\n",
    "torch_model = torch_model_runner(torch.load(f\"{prefix}/some_model.pt\", map_location=torch.device('cuda:0')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 152\u001b[0m\n\u001b[1;32m    149\u001b[0m   ress \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(res)\n\u001b[1;32m    150\u001b[0m   \u001b[39mreturn\u001b[39;00m ress\n\u001b[0;32m--> 152\u001b[0m infer_optimal(params, \u001b[39m\"\u001b[39;49m\u001b[39mкек\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[2], line 117\u001b[0m, in \u001b[0;36minfer_optimal\u001b[0;34m(params, text)\u001b[0m\n\u001b[1;32m    114\u001b[0m   prediction \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m$skip\u001b[39m\u001b[39m\"\u001b[39m \n\u001b[1;32m    115\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m   \u001b[39m# params[\"ID_TO_PUNCTUATION\"], output_probas\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m   prediction \u001b[39m=\u001b[39m predict_on_tokens(window_left, window_right, return_probas\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \n\u001b[1;32m    120\u001b[0m \u001b[39m#random.choice([\" \", \".\"])\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m log: \u001b[39mprint\u001b[39m(d_as_str(window_left)\u001b[39m.\u001b[39mrjust(\u001b[39m100\u001b[39m), prediction\u001b[39m.\u001b[39mcenter(\u001b[39m6\u001b[39m), d_as_str(window_right))\n",
      "Cell \u001b[0;32mIn[2], line 83\u001b[0m, in \u001b[0;36minfer_optimal.<locals>.predict_on_tokens\u001b[0;34m(window_left, window_right, return_probas)\u001b[0m\n\u001b[1;32m     81\u001b[0m features_for_batch \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack((features, ))\n\u001b[1;32m     82\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mascontiguousarray(features_for_batch, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m---> 83\u001b[0m output_probas \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray((jsinfer\u001b[39m.\u001b[39;49minfer(arr))\u001b[39m.\u001b[39;49mto_py())\n\u001b[1;32m     84\u001b[0m \u001b[39m# output_probas[0][0] += 2.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39mif\u001b[39;00m return_probas:\n",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m, in \u001b[0;36mjsinfer.infer.<locals>.wrapper.to_py\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_py\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_model(torch\u001b[39m.\u001b[39mfrom_numpy(arr)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch_model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pymorphy3\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from razdel import tokenize\n",
    "\n",
    "from dataset_builder import calculate_word_features_for_tokens, PAD_TOKEN,get_word_features\n",
    "from inference import torch_model_runner, onnx_model_runner, infer\n",
    "\n",
    "onnx_model = onnx_model_runner(\"results_big_model_GOOD_2022-05-15/model.onnx\")\n",
    "with open(\"params.pickle\", \"rb\") as f:\n",
    "    params = pickle.load(f)\n",
    "\n",
    "# class jsinfer:\n",
    "#     async def infer(arr):\n",
    "#         class wrapper:\n",
    "#             def to_py():\n",
    "#                 return onnx_model(arr)\n",
    "        # return wrapper\n",
    "    \n",
    "\n",
    "class jsinfer:\n",
    "    def infer(arr):\n",
    "        class wrapper:\n",
    "            def to_py():\n",
    "                return torch_model(torch.from_numpy(arr).to(\"cuda:0\")).cpu().numpy()\n",
    "        return wrapper\n",
    "\n",
    "\n",
    "from stream import Stream\n",
    "import functools\n",
    "from collections import deque\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "class Substr:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Substring(-1, -1, {self.text})\"\n",
    "\n",
    "def d_as_str(d):\n",
    "  return \"<\" + \" \".join(map(lambda text: text.text, d))+ \">\"\n",
    "    \n",
    "def infer_optimal(params, text): \n",
    "  @functools.lru_cache(maxsize=128)\n",
    "  def get_word_features_cached(word):\n",
    "      return get_word_features(word, params).numpy()\n",
    "          \n",
    "  # print(\"INFERCENC IS WIERD\\n\" * 10)\n",
    "  res = []\n",
    "  last_inserted_pos = 0\n",
    "  def sink(token, log=False):\n",
    "    nonlocal last_inserted_pos\n",
    "    if token.text == \"PAD\": return\n",
    "    if log: print('sink', token)\n",
    "    if isinstance(token, Substr):\n",
    "      res.append(token.text)\n",
    "      if log: print(\"added1 \", f\"`{token.text}`\", token)\n",
    "    else:\n",
    "      if last_inserted_pos != token.start:\n",
    "        res.append(text[last_inserted_pos: token.start])\n",
    "        if log: print(\"added2 \", f\"`{text[last_inserted_pos: token.start]}`\", last_inserted_pos, token.start)\n",
    "      last_inserted_pos = token.stop\n",
    "      res.append(token.text)\n",
    "      if log: print(\"added3 \", f\"`{token.text}`\", token)\n",
    "\n",
    "  def skip(token, log=False):\n",
    "    nonlocal last_inserted_pos\n",
    "    last_inserted_pos = token.stop\n",
    "    if log: print('skip', token)\n",
    "\n",
    "  def sink_remaining():\n",
    "     res.append(text[last_inserted_pos:])\n",
    "\n",
    "\n",
    "  def predict_on_tokens(window_left, window_right, return_probas):\n",
    "    features = [get_word_features_cached(i.text) for i in Stream(window_left).chain(window_right)]\n",
    "    features_for_batch = np.stack((features, ))\n",
    "    arr = np.ascontiguousarray(features_for_batch, dtype=np.float32)\n",
    "    output_probas = np.array((jsinfer.infer(arr)).to_py())\n",
    "    # output_probas[0][0] += 2.\n",
    "    if return_probas:\n",
    "      return params[\"ID_TO_PUNCTUATION\"], output_probas \n",
    "    punct_idx = np.argmax(output_probas).item()\n",
    "    punct = params[\"ID_TO_PUNCTUATION\"][punct_idx]\n",
    "    return punct\n",
    "\n",
    "\n",
    "  window_left = deque()\n",
    "  window_right = deque()\n",
    "  log = False\n",
    "  skip_next = False\n",
    "  for i in Stream.repeat(Substr(PAD_TOKEN), params['INPUT_WORDS_CNT_LEFT']) \\\n",
    "      .chain(Stream(tokenize(text))) \\\n",
    "      .chain(Stream.repeat(Substr(PAD_TOKEN), params[\"INPUT_WORDS_CNT_RIGHT\"])):\n",
    "    window_right.append(i)\n",
    "    if len(window_right) <= params[\"INPUT_WORDS_CNT_RIGHT\"]:\n",
    "        continue\n",
    "    assert len(window_right) == params[\"INPUT_WORDS_CNT_RIGHT\"] + 1\n",
    "\n",
    "    next_ = window_right.popleft()\n",
    "    sink(next_)\n",
    "    window_left.append(next_)\n",
    "    if len(window_left) < params['INPUT_WORDS_CNT_LEFT']: \n",
    "      continue\n",
    "\n",
    "    assert len(window_left) == params[\"INPUT_WORDS_CNT_LEFT\"]\n",
    "    assert len(window_right) == params[\"INPUT_WORDS_CNT_RIGHT\"]\n",
    "\n",
    "    if skip_next:\n",
    "      prediction = \"$skip\" \n",
    "    else:\n",
    "      # params[\"ID_TO_PUNCTUATION\"], output_probas\n",
    "      prediction = predict_on_tokens(window_left, window_right, return_probas=False) \n",
    "\n",
    "\n",
    "    #random.choice([\" \", \".\"])\n",
    "    if log: print(d_as_str(window_left).rjust(100), prediction.center(6), d_as_str(window_right))\n",
    "\n",
    "    def is_replaceable_punct(punct):\n",
    "      return punct in ',.'\n",
    "\n",
    "    if prediction == \"$skip\":\n",
    "      pass\n",
    "    elif prediction != \"$empty\":\n",
    "      if is_replaceable_punct(window_right[0].text):\n",
    "        if window_right[0].text != prediction:\n",
    "          window_right[0].text = prediction\n",
    "      else:\n",
    "        window_left.append(Substr(prediction))\n",
    "        sink(window_left[-1])\n",
    "    else:\n",
    "      if is_replaceable_punct(window_right[0].text):\n",
    "          skip(window_right.popleft())\n",
    "\n",
    "    skip_next = is_replaceable_punct(window_right[0].text)\n",
    "\n",
    "    while len(window_left) != params['INPUT_WORDS_CNT_LEFT'] - 1: \n",
    "      token = window_left.popleft()\n",
    "\n",
    "    if log: print(d_as_str(window_left).rjust(100), \"      \", d_as_str(window_right))\n",
    "\n",
    "  for i in window_right:\n",
    "    sink(i)\n",
    "  sink_remaining()\n",
    "  ress = \"\".join(res)\n",
    "  return ress\n",
    "\n",
    "infer_optimal(params, \"кек\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'кек.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_optimal(params, \"кек\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.calculate_diff.<locals>.<lambda>()>,\n",
       "            {'unchanged .': 0, 'unchanged ,': 0, 'add .': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import diff_match_patch as dmp_module\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def calculate_diff(text, text_res):\n",
    "  dmp = dmp_module.diff_match_patch()\n",
    "  diff = dmp.diff_main(text, text_res)\n",
    "\n",
    "  diff_aggregate = defaultdict(lambda : 0)\n",
    "  def sink2():\n",
    "    nonlocal cur_add, cur_remove\n",
    "    if cur_add == \"\" and cur_remove == \"\":\n",
    "      return\n",
    "    if cur_add == \"\":\n",
    "      diff_aggregate['remove ' + cur_remove] += 1\n",
    "      cur_remove = \"\"\n",
    "      return\n",
    "    if cur_remove == \"\":\n",
    "      diff_aggregate['add ' + cur_add] += 1\n",
    "      cur_add = \"\"\n",
    "      return\n",
    "    \n",
    "    diff_aggregate['replace ' + cur_remove + \" with \" + cur_add] += 1\n",
    "    cur_add = \"\"\n",
    "    cur_remove = \"\"\n",
    "\n",
    "  cur_remove = \"\"\n",
    "  cur_add = \"\"\n",
    "\n",
    "  UNCHANGED = 0\n",
    "  ADD = 1\n",
    "  REMOVE = -1\n",
    "\n",
    "  for change in diff:\n",
    "      if change[0] == UNCHANGED:\n",
    "        c = Counter(change[1])\n",
    "        diff_aggregate['unchanged .'] += c['.']\n",
    "        diff_aggregate['unchanged ,'] += c[',']\n",
    "        sink2() \n",
    "      elif change[0] == ADD:\n",
    "        cur_add += change[1]\n",
    "      elif change[0] == REMOVE:\n",
    "        cur_remove += change[1]\n",
    "      else:\n",
    "        raise Exception(\"Unknown format\")\n",
    "      \n",
    "  sink2()\n",
    "\n",
    "  return diff_aggregate\n",
    " \n",
    "text = \"кек\"\n",
    "calculate_diff(text, await infer_optimal(params, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.calculate_diff2.<locals>.<lambda>()>,\n",
       "            {'changed . with ,': 1,\n",
       "             'not changed ,': 1,\n",
       "             'not changed .': 1,\n",
       "             'added .': 1,\n",
       "             'possible punctuation places': 7})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "text = \"Тест. тест, тест. Тест\"\n",
    "text_res = infer_optimal(params, text)\n",
    "\n",
    "def calculate_diff2(text, text_res):\n",
    "  res = defaultdict(lambda: 0)\n",
    "\n",
    "  def is_punctuation(c):\n",
    "      return c in \".,\"\n",
    "\n",
    "  def sink_add(c):\n",
    "    nonlocal res\n",
    "    res['added ' + c] += 1\n",
    "\n",
    "  def sink_remove(c):\n",
    "    nonlocal res\n",
    "    res['removed ' + c] += 1\n",
    "\n",
    "  def sink_change(c1, c2):\n",
    "    nonlocal res\n",
    "    res['changed ' + c1 + \" with \" + c2] += 1\n",
    "\n",
    "  i = 0\n",
    "  j = 0\n",
    "  while True:\n",
    "      if i >= len(text): break\n",
    "      if j >= len(text_res): break\n",
    "      # print(text[i], text_res[j])\n",
    "      if text[i] == text_res[j]:\n",
    "          if is_punctuation(text[i]):\n",
    "             res['not changed ' + text[i]] += 1\n",
    "          i += 1\n",
    "          j += 1\n",
    "          continue\n",
    "      \n",
    "      if is_punctuation(text[i]) and is_punctuation(text_res[j]):\n",
    "        sink_change(text[i], text_res[j])\n",
    "        i += 1\n",
    "        j += 1\n",
    "        continue\n",
    "      \n",
    "      if is_punctuation(text[i]):\n",
    "        sink_remove(text[i])\n",
    "        i += 1\n",
    "        continue\n",
    "      \n",
    "      if is_punctuation(text_res[j]):\n",
    "        sink_add(text_res[j])\n",
    "        j += 1\n",
    "        continue\n",
    "      \n",
    "      raise Exception(\"Change not in punctuation\", text[i], text_res[j], \"at \", i, j)\n",
    "\n",
    "  while i < len(text):\n",
    "    # print(\"remaining: \", text[i])\n",
    "    assert is_punctuation(text[i])\n",
    "    sink_remove(text[i])\n",
    "    i += 1\n",
    "\n",
    "  while j < len(text_res):\n",
    "    # print(\"remaining(2): \",text_res[j])\n",
    "    assert is_punctuation(text_res[j])\n",
    "    sink_add(text_res[j])\n",
    "    j += 1\n",
    "\n",
    "  res['possible punctuation places'] = len(list(tokenize(text)))\n",
    "\n",
    "  return res\n",
    "\n",
    "calculate_diff2(text, text_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f284334e594d55a133785b959ef6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Either FairScale or torch distributed is not available, MixtureOfExperts will not be exposed. Please install them if you would like to use MoE\n",
      "Either FairScale or torch distributed is not available, MixtureOfExperts will not be exposed. Please install them if you would like to use MoE\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m rtf_path \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(glob\u001b[39m.\u001b[39mglob(\u001b[39m\"\u001b[39m\u001b[39m../validation/Mark Tven/Mark Tven rtf/*.rtf\u001b[39m\u001b[39m\"\u001b[39m))[:\u001b[39m2\u001b[39m]:\n\u001b[1;32m     23\u001b[0m   tasks\u001b[39m.\u001b[39mappend(joblib\u001b[39m.\u001b[39mdelayed(parse_file)(rtf_path))\n\u001b[0;32m---> 24\u001b[0m completed_tasks \u001b[39m=\u001b[39m ProgressParallel(n_jobs\u001b[39m=\u001b[39;49mjoblib\u001b[39m.\u001b[39;49mcpu_count(), total\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(tasks))(tasks)\n\u001b[1;32m     26\u001b[0m res \u001b[39m=\u001b[39m defaultdict(\u001b[39mlambda\u001b[39;00m: \u001b[39m0\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdicts_sum\u001b[39m(dict1, dict2):\n",
      "File \u001b[0;32m~/audio-ml/lib/utils.py:25\u001b[0m, in \u001b[0;36mProgressParallel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     24\u001b[0m     \u001b[39mwith\u001b[39;00m tqdm(disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_tqdm, total\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_total) \u001b[39mas\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pbar:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m Parallel\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/pytorch-env/envs/pytorch-env/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/micromamba/envs/pytorch-env/envs/pytorch-env/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/micromamba/envs/pytorch-env/envs/pytorch-env/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/pytorch-env/envs/pytorch-env/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/micromamba/envs/pytorch-env/envs/pytorch-env/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "# from tqdm.notebook import tqdm\n",
    "from utils import ProgressParallel\n",
    "import joblib\n",
    "\n",
    "def parse_file(rtf_path):\n",
    "  with open(rtf_path, \"rb\") as rtf_file:\n",
    "    encoded = rtf_file.read()\n",
    "    try:\n",
    "      rtf = encoded.decode('cp1251')\n",
    "      txt = rtf_to_text(rtf)\n",
    "      diff = calculate_diff2(txt, infer_optimal(params, txt))\n",
    "    except Exception as ex:\n",
    "      print(\"skipped \", rtf_path, len(encoded), ex)\n",
    "      # raise\n",
    "      return {}\n",
    "\n",
    "\n",
    "tasks = []\n",
    "\n",
    "for rtf_path in list(glob.glob(\"../validation/Mark Tven/Mark Tven rtf/*.rtf\"))[:2]:\n",
    "  tasks.append(joblib.delayed(parse_file)(rtf_path))\n",
    "completed_tasks = ProgressParallel(n_jobs=joblib.cpu_count(), total=len(tasks))(tasks)\n",
    "\n",
    "res = defaultdict(lambda: 0)\n",
    "def dicts_sum(dict1, dict2):\n",
    "  for key in dict2:\n",
    "    dict1[key] += dict2[key]\n",
    "  return dict1\n",
    "\n",
    "for diff in completed_tasks:\n",
    "  res = dicts_sum(res, diff)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>, {})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
